<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>CLoVE</title>
    <link href="./css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.1/font/bootstrap-icons.css">

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN" crossorigin="anonymous">
  </head>
  <body>

    <div class="container">
        <div class="row">
            <article>
                <h1 class="display-4">
                    Self-supervised Learning of Contextualized Local Visual Embeddings
                </h1>
                <p class="fs-4">
                    Thalles Santos Silva, Helio Pedrini, Adín Ramírez Rivera
                </p>
                <p class="fs-5">
                    4th Visual Inductive Priors for Data-Efficient Deep Learning Workshop
                </p>
            </article>
            <!-- <img src="./images/carp_overview.png" class="img-fluid" alt="CARP overview architecture."> -->
            <img src="./images/clove_overview.png" class="rounded float-start" alt="...">
            <!-- <img src="./images/carp_rp.png" class="rounded float-end" alt="..."> -->

        </div>
        <div class="row">
            <h2 class="display-6">Abstract</h2>
            <p class="fst-italic">
                We present Contextualized Local Visual Embeddings (CLoVE), a self-supervised convolutional-based method that learns representations suited for dense prediction tasks. CLoVE deviates from current methods and optimizes a single loss function that operates at the level of contextualized local embeddings learned from output feature maps of convolution neural network (CNN) encoders. To learn contextualized embeddings, CLoVE proposes a normalized multi-head self-attention layer that combines local features from different parts of an image based on similarity. We extensively benchmark CLoVE’s pre-trained representations on multiple datasets. CLoVE reaches state-of-the-art performance for CNN-based architectures in 4 dense prediction downstream tasks, including object detection, instance segmentation, keypoint detection, and dense pose estimation.
            </p>
        </div>

        <div class="row">
            <h2 class="display-6">Pre-trained models</h2>
            <p>Models trained with ResNet50 encoders.</p>
            <p>TODO</p>
            <!-- <table class="table">
                <thead>
                  <tr>
                    <th scope="col"></th>
                    <th scope="col">Epochs</th>
                    <th scope="col">Multicrop</th>
                    <th scope="col">Linear</th>
                    <th scope="col">K-NN</th>
                    <th scope="col">URL</th>
                  </tr>
                </thead>
                <tbody class="table-group-divider">
                  <tr>
                    <th scope="row">CARP</th>
                    <td>200</td>
                    <td>2x224 + 6x96</td>
                    <td>74.0</td>
                    <td>66.5</td>
                    <td><p><a class="link-opacity-10-hover" href="https://drive.google.com/drive/folders/1ah5ACDMbLQSRy621JH9UrmiU1QAEYOE9?usp=sharing">checkpoints</a></p></td>
                  </tr>
                  <tr>
                    <th scope="row">CARP</th>
                    <td>400</td>
                    <td>2x224</td>
                    <td>70.4</td>
                    <td>62.8</td>
                    <td><p><a class="link-opacity-10-hover" href="https://drive.google.com/drive/folders/1ah5ACDMbLQSRy621JH9UrmiU1QAEYOE9?usp=drive_link">checkpoints</a></p></td>
                  </tr>
                  <tr>
                    <th scope="row">CARP</th>
                    <td>400</td>
                    <td>2x224 + 6x96</td>
                    <td>75.0</td>
                    <td>67.7</td>
                    <td><p><a class="link-opacity-10-hover" href="https://drive.google.com/drive/folders/1rCfwBQu9njtJ5q9FO-ZmU4n9AXtNepuu?usp=drive_link">checkpoints</a></p></td>
                  </tr>
                </tbody>
              </table> -->
        </div>

        <div class="row">
            <h2 class="display-6">Performance</h2>
            <img src="./images/clove_results.png" class="rounded float-start" alt="...">
        </div>

        <div class="row">
            <h2 class="display-6">Important links</h2>
            
            <table class="table">
                <thead>
                  <tr>
                    <th scope="col">
                        <a class="icon-link" href="https://github.com/sthalles/CLoVE">
                            <i class="bi-code" style="font-size: 2rem; color: cornflowerblue;"></i>
                            Code
                        </a>
                    </th>
                    <th scope="col">
                        <a class="icon-link" href="https://arxiv.org/abs/2310.00527">
                            <i class="bi-files" style="font-size: 2rem; color: cornflowerblue;"></i>
                            arXiv
                        </a>
                    </th>
                    <!-- <th scope="col">
                        <a class="icon-link" href="#">
                            <i class="bi-paperclip" style="font-size: 2rem; color: cornflowerblue;"></i>
                            NeurIPS 2023 Proceedings
                        </a>
                    </th> -->
                    <th scope="col">
                        <a class="icon-link" href="https://openreview.net/forum?id=wGCimyErJ6">
                            <i class="bi-paperclip" style="font-size: 2rem; color: cornflowerblue;"></i>
                            OpenReview
                        </a>
                    </th>
                  </tr>
                </thead>
              </table>
        </div>

        <div class="row">

            <h2 class="display-6">Reference</h2>

            <pre class="citation"><code >
@inproceedings{silva2022representation,
    title={Representation learning via consistent assignment of views to clusters},
    author={Silva, Thalles and Rivera, Ad{\'\i}n Ram{\'\i}rez},
    booktitle={Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
    pages={987--994},
    year={2022}
}
            </code></pre>

        </div>

        <div class="row">
            <h2 class="display-6">Authors</h2>
            <div class="col">
                <div class="span4"></div>
                <div class="span4">
                    <img class="author-profile-images center-block img-thumbnail rounded-circle" src="./images/thalles.jpeg" />
                    <p class="text-center">Thalles Santos Silva</p>
                </div>
                <div class="span4"></div>
            </div>
            <div class="col">
                <div class="span4"></div>
                <div class="span4">
                    <img class="author-profile-images center-block img-thumbnail rounded-circle" src="./images/Helio.jpg" />
                    <p class="text-center">Helio Pedrini</p>
                </div>
                <div class="span4"></div>
            </div>
            <div class="col">
                <div class="span4"></div>
                <div class="span4">
                    <img class="author-profile-images center-block img-thumbnail rounded-circle" src="./images/adin.jpg" />
                    <p class="text-center">Adín Ramírez River</p>
                </div>
                <div class="span4"></div>
            </div>
           
        </div>

        <div class="row">
            <h2 class="display-6">Acknowledgements</h2>
            <p>
                The computations were performed in part on resources provided by Sigma2---the National Infrastructure for High Performance Computing and Data Storage in Norway---through Project NN8104K.
                This work was funded in part by the Research Council of Norway, through its Centre for Research-based Innovation funding scheme (grant no. 309439), and Consortium Partners.
                
                This study was financed in part by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior---Brasil (CAPES)---Finance Code 001
            </p>
        </div>

        <div class="row">

            <table class="table">
                <thead>
                  <tr>
                    <th scope="col">
                        <img class="logo center-block" src="./images/unicamp.png" />
                    </th>
                    <th scope="col">
                        <img class="logo center-block" src="./images/ic.png" />
                    </th>
                    <th scope="col">
                        <img class="logo center-block" src="./images/recod.png" />
                    </th>
                    <th scope="col">
                        <img class="logo center-block" src="./images/uio_logo.png" />
                    </th>
                  </tr>
                </thead>
              </table>

        </div>

    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL" crossorigin="anonymous"></script>
  </body>
</html>